import streamlit as st
from agents.ingestion_agent import IngestionAgent
from agents.retrieval_agent import RetrievalAgent
from agents.llm_response_agent import LLMResponseAgent
from agents.mcp import MCPMessage
import os

st.set_page_config(page_title="Agentic RAG with MCP")
st.title("Agentic RAG Chatbot with MCP")


if "agents_initialized" not in st.session_state:
    st.session_state.ingestion = IngestionAgent()
    st.session_state.retrieval = RetrievalAgent()
    st.session_state.llm = LLMResponseAgent()
    st.session_state.agents_initialized = True


uploaded_files = st.file_uploader("Upload docs", type=["pdf", "csv", "docx", "pptx", "txt", "md"], accept_multiple_files=True)

if uploaded_files:
    file_paths = []
    os.makedirs("temp", exist_ok=True)

    for file in uploaded_files:
        path = os.path.join("temp", file.name)
        with open(path, "wb") as f:
            f.write(file.getbuffer())
        file_paths.append(path)


    msg = MCPMessage("UI", "IngestionAgent", "UPLOAD", {"files": file_paths})
    chunks_msg = st.session_state.ingestion.handle(msg)
    ready_msg = st.session_state.retrieval.handle(MCPMessage.from_dict(chunks_msg))

    st.success("Files embedded and vector store is ready.")


user_query = st.text_input("Ask a question from your document")

if st.button("Ask"):
    if user_query:
        query_msg = MCPMessage("UI", "RetrievalAgent", "QUERY", {"query": user_query})
        context_msg = st.session_state.retrieval.handle(query_msg)
        answer_msg = st.session_state.llm.handle(MCPMessage.from_dict(context_msg))

        st.subheader("Answer:")
        st.write(answer_msg["payload"]["answer"])

        st.subheader("Source Context:")
        for c in answer_msg["payload"]["context"]:
            st.info(c)
